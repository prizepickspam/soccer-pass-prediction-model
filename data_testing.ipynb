{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e0711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e4b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import data\n",
    "importlib.reload(data)\n",
    "from data import DataLoader\n",
    "\n",
    "prem2425 = '9n12waklv005j8r32sfjj2eqc'\n",
    "\n",
    "# prem_loader = DataLoader(prem2425)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc7a08d",
   "metadata": {},
   "source": [
    "# statsperform API + bq backloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19de6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _access_statsperform_api(feed_name: str,\n",
    "                             tourney_cal_id: str = None,\n",
    "                             match_id: str = None\n",
    "                             ):\n",
    "\n",
    "    proxy_url = \"http://127.0.0.1:3128\"\n",
    "    statsperform_base_url = 'https://api.performfeeds.com/soccerdata'\n",
    "    auth_key = str(os.environ['STATSPERFORM_API_KEY'])\n",
    "\n",
    "    master_dict = {\n",
    "        '_rt':'b',\n",
    "        '_fmt':'json'\n",
    "    }\n",
    "\n",
    "    if feed_name == 'match':\n",
    "        assert tourney_cal_id is not None, \"To access match feed data, a tournament calendar ID must be passed in.\"\n",
    "        master_dict['tmcl'] = tourney_cal_id\n",
    "        master_dict['_pgSz'] = 1000\n",
    "    if feed_name == 'matchstats':\n",
    "        assert match_id is not None, \"To access match stats feed, a match ID must be passed in.\"\n",
    "        master_dict['fx'] = match_id\n",
    "        master_dict['detailed'] = 'yes'\n",
    "        master_dict['people'] = 'yes'\n",
    "\n",
    "    query_string = '&'.join([f'{k}={v}' for k, v in master_dict.items()])\n",
    "\n",
    "    q_command = f\"\"\" curl -x \"{proxy_url}\" '{statsperform_base_url}/{feed_name}/{auth_key}/authorized?&{query_string}' \"\"\" if feed_name == 'tournamentcalendar' else \\\n",
    "        f\"\"\" curl -x \"{proxy_url}\" '{statsperform_base_url}/{feed_name}/{auth_key}?&{query_string}' \"\"\"\n",
    "\n",
    "    # print(q_command)\n",
    "\n",
    "    process = subprocess.run(\n",
    "                q_command,\n",
    "                shell=True,\n",
    "                capture_output=True,\n",
    "                check=False # Set to True if you want a CalledProcessError for non-zero exit codes\n",
    "            )\n",
    "\n",
    "    stdout = process.stdout\n",
    "    json_output = json.loads(stdout)\n",
    "    return json_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba30d6b",
   "metadata": {},
   "source": [
    "## tournaments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2049e914",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_statsperform_tourneys(to_bq=False):\n",
    "    # Get all available tournament calendar IDs with OT2 feed\n",
    "    feed = 'tournamentcalendar'\n",
    "    comps = _access_statsperform_api(feed)\n",
    "    competitions = pd.DataFrame(comps['competition'])\n",
    "    competitions = competitions.explode('tournamentCalendar')\n",
    "\n",
    "    extended_tourneys = pd.json_normalize(competitions['tournamentCalendar'])\n",
    "    extended_tourneys.columns = ['tourneyCalId', 'includesVenues', 'tcOcId', 'tcName', 'startDate', 'endDate', 'active', 'lastUpdated', 'includesStandings']\n",
    "\n",
    "    final_df = pd.concat([competitions.drop(columns=['tournamentCalendar']).reset_index(drop=True),\n",
    "                      extended_tourneys.reset_index(drop=True)], axis=1)\n",
    "                                \n",
    "    if to_bq:\n",
    "        try:\n",
    "            final_df.to_gbq('soccer_simulations.tourneycal_data',\n",
    "                    'prizepicksanalytics',\n",
    "                    if_exists='fail')\n",
    "        except Exception as e:\n",
    "            print(\"Table already exists on BQ!\")\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296971d8",
   "metadata": {},
   "source": [
    "## matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254af8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all matches with MA1 feed\n",
    "def _get_all_matches_in_tourneycal(tourney_cal_id: str):\n",
    "\n",
    "    all_matches = _access_statsperform_api(feed_name='match',\n",
    "                         tourney_cal_id=tourney_cal_id)\n",
    "\n",
    "    return pd.DataFrame([x['matchInfo'] for x in all_matches['match']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cf3f14",
   "metadata": {},
   "source": [
    "## process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caba185",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = _get_all_matches_in_tourneycal(tourney_cal_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80fb8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "import config\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cafe3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "teams, players = backload_season_data(tourney_cal_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6646ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# getting access to player data using MA2 feed\n",
    "def backload_season_data(tourney_cal_id, \n",
    "                         game_limit=None,\n",
    "                         project_name: str = 'prizepicksanalytics',\n",
    "                        table_name: str = f'soccer_simulations.schema_match_data'):\n",
    "    \n",
    "    matches = _get_all_matches_in_tourneycal(tourney_cal_id)\n",
    "\n",
    "    processed_match_ids = list(get_processed_match_ids().match_id)\n",
    "    list_of_matches = [x for x in matches.id.unique() if x not in processed_match_ids]\n",
    "\n",
    "    if game_limit:\n",
    "        list_of_matches = list_of_matches[:game_limit]\n",
    "\n",
    "    all_teams = []\n",
    "    all_players = []\n",
    "\n",
    "    for i, id in enumerate(list_of_matches):\n",
    "\n",
    "        if id in processed_match_ids:\n",
    "            continue\n",
    "\n",
    "        print(f\"Now processing match #{i}: {id}\")\n",
    "        team, player = process_game_data(id)\n",
    "\n",
    "        all_players.append(player)\n",
    "        all_teams.append(team)\n",
    "        \n",
    "\n",
    "    if all_teams and all_players:\n",
    "        team_data_to_bq = pd.concat(all_teams, axis=0, ignore_index=True)\n",
    "        team_data_to_bq.to_gbq(table_name.replace('schema', 'team'),\n",
    "                project_name,\n",
    "                if_exists='append')\n",
    "        \n",
    "        player_data_to_bq = pd.concat(all_players, axis=0, ignore_index=True)\n",
    "        player_data_to_bq.to_gbq(table_name.replace('schema', 'player'),\n",
    "                project_name,\n",
    "                if_exists='append')\n",
    "        \n",
    "        print(f\"Team and Player Data Uploaded for the following Game Ids: {team_data_to_bq.match_id.unique()}\")\n",
    "        return team, player\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03037793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# players.to_csv('./test_players.csv', index=False)\n",
    "# teams.to_csv('./test_teams.csv', index=False)\n",
    "# players = pd.read_csv('./test_players.csv', index_col=0)\n",
    "# teams = pd.read_csv('./test_teams.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a95733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery as bqc\n",
    "\n",
    "\n",
    "# DataFrame should be of only one match\n",
    "def check_bq_duplicates(dataframe: pd.DataFrame,\n",
    "                        schema: str,\n",
    "                        project_name: str = 'prizepicksanalytics',\n",
    "                        table_name: str = f'soccer_simulations.schema_match_data'):\n",
    "    \n",
    "    assert dataframe.shape[0] != 0, \"Empty dataframes not accepted\"\n",
    "    assert schema in ['team', 'player'], \"Only team and player data accepted\"\n",
    "    table_name = table_name.replace('schema', schema)\n",
    "\n",
    "    match_ids = list(dataframe.match_id.unique())\n",
    "\n",
    "    column_key = 'team_id' if schema == 'team' else 'playerId'\n",
    "\n",
    "    query = f\"\"\"\n",
    "        select match_id, {column_key}\n",
    "        from {project_name}.{table_name}\n",
    "        where match_id in ({\", \".join([\"'\" + x + \"'\" for x in match_ids])}) \n",
    "    \"\"\"\n",
    "\n",
    "    rows_on_bq = execute_bq_query(query)\n",
    "\n",
    "    if rows_on_bq.shape[0] == dataframe.shape[0]:\n",
    "        print(f\"Data already loaded onto BQ for Match Id: {match_ids} Schema: {schema}\")\n",
    "        return\n",
    "    else:\n",
    "        dataframe = dataframe.merge(rows_on_bq,\n",
    "                                    on=['match_id', column_key],\n",
    "                                    how='left',\n",
    "                                    indicator=True)\n",
    "        \n",
    "        dataframe = dataframe[dataframe['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "        \n",
    "\n",
    "    return dataframe\n",
    "\n",
    "def get_processed_match_ids():\n",
    "    query = \"\"\"\n",
    "        select distinct match_id\n",
    "        from prizepicksanalytics.soccer_simulations.team_match_data\n",
    "    \"\"\"\n",
    "\n",
    "    return execute_bq_query(query)\n",
    "\n",
    "\n",
    "def execute_bq_query(query: str) -> pd.DataFrame:\n",
    "    bigquery_client = bqc.Client(project='prizepicksanalytics')\n",
    "    query_results = bigquery_client.query(query)\n",
    "    data = query_results.to_dataframe()\n",
    "    bigquery_client.close()\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47cfd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_game_data(match_id):\n",
    "    \n",
    "    match_data = _access_statsperform_api(feed_name='matchstats',\n",
    "                         match_id=match_id)\n",
    "    \n",
    "    # TODO: \n",
    "    team_data = _aggregate_team_data(match_data)\n",
    "\n",
    "    player_data = _aggregate_player_data(match_data)\n",
    "    \n",
    "    return team_data, player_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a75fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to aggregate team data\n",
    "\n",
    "def _aggregate_team_data(match_stats):\n",
    "\n",
    "    # collect tournament calendar, competition and team information\n",
    "    tourney_cal_id, tourney_cal_season = match_stats['matchInfo']['tournamentCalendar']['id'], \\\n",
    "        match_stats['matchInfo']['tournamentCalendar']['name']\n",
    "    competition_id, competition_name = match_stats['matchInfo']['competition']['id'], \\\n",
    "        match_stats['matchInfo']['competition']['name']\n",
    "    \n",
    "    competitors = match_stats['matchInfo']['contestant']\n",
    "    \n",
    "    # turn match data into dataframe\n",
    "    match_data = pd.DataFrame(match_stats['liveData']['lineUp'])\n",
    "\n",
    "    match_list = []\n",
    "\n",
    "    for _, team in match_data.iterrows():\n",
    "        # turn statistics into numerics and get team object from competitor\n",
    "        team_stats = pd.DataFrame(team.stat)\n",
    "        team_stats['value'] = pd.to_numeric(team_stats.value)\n",
    "        competitor = competitors[0] if competitors[0]['id'] == team.contestantId else competitors[1]\n",
    "\n",
    "        # transpose statistics to make ts dataframe (team statistics)\n",
    "        ts = team_stats[['type', 'value']].set_index('type').T\n",
    "\n",
    "        stat_cols = ts.columns\n",
    "        missing_cols = set(config.all_team_match_stats).difference(stat_cols)\n",
    "\n",
    "        if missing_cols:\n",
    "            missing_df = pd.DataFrame(0, index=ts.index, columns=list(missing_cols), dtype=float)\n",
    "            ts = pd.concat([ts, missing_df], axis=1)\n",
    "\n",
    "        # create dataframe for cs (config statistics)\n",
    "        cs = pd.DataFrame({\n",
    "            'competition_id': [competition_id],\n",
    "            'competition_name': [competition_name],\n",
    "            'tourney_cal_id': [tourney_cal_id],\n",
    "            'tourney_cal_name': [tourney_cal_season],\n",
    "            'match_id' : [match_stats['matchInfo']['id']],\n",
    "            'match_date': match_stats['matchInfo']['date'],\n",
    "            'team_id' : [str(competitor['id'])],\n",
    "            'team_name': [str(competitor['shortName'])],\n",
    "            'home' : True if competitor['position'] == 'home' else False,\n",
    "        })\n",
    "\n",
    "        team_row = pd.concat([cs.reset_index(drop=True), ts[config.all_team_match_stats].reset_index(drop=True)], axis=1)\n",
    "        team_row['formationUsed'] = str(team.formationUsed)\n",
    "\n",
    "        match_list.append(team_row)\n",
    "\n",
    "    # return dataframe of len == 2 with rows representing both teams, sharing the same game_id\n",
    "    return pd.concat(match_list).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027c50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to aggregate player data\n",
    "\n",
    "def _aggregate_player_data(match_request):\n",
    "    match_data = pd.DataFrame(match_request['liveData']['lineUp'])\n",
    "\n",
    "    subs = pd.DataFrame(match_request['liveData']['substitute']).set_index('playerOnId').to_dict('index')\n",
    "\n",
    "    list_of_dfs = []\n",
    "\n",
    "    for _, team in match_data.iterrows():\n",
    "        players = pd.DataFrame(team['player'])\n",
    "\n",
    "        # replace substitution position with boolean column\n",
    "        players['isSub'] = players.position.apply(lambda x: True if x == 'Substitute' else False)\n",
    "        cols_to_keep = ['playerId', 'matchName', 'position', 'positionSide', 'isSub']\n",
    "\n",
    "        # turn statistics json into pandas dataframe + make all numbers into floats\n",
    "        stats = players.stat.apply(lambda lisa: {x['type']:float(x['value']) for x in lisa})\n",
    "        players = pd.concat([players[cols_to_keep].reset_index(drop=True), pd.json_normalize(stats).reset_index(drop=True)], axis=1)\n",
    "\n",
    "        # remove all players from dataframe who didn't play\n",
    "        players = players[~players['minsPlayed'].isna()]\n",
    "\n",
    "        # checks all subs in players dataframe, replace values with subbed out player info\n",
    "        subbed_players = players[players['isSub'] == True]\n",
    "\n",
    "        # replace substitution position info with player who subbed out for substitute\n",
    "        for i, row in subbed_players.iterrows():\n",
    "            sub_info = subs[row.playerId]\n",
    "            subbed_out_player = players[players['playerId'] == sub_info['playerOffId']].iloc[0]\n",
    "\n",
    "            players.at[i, 'position'] =  subbed_out_player.position\n",
    "            players.at[i, 'positionSide'] =  subbed_out_player.positionSide\n",
    "            players.at[i, 'formationPlace'] =  subbed_out_player.formationPlace\n",
    "\n",
    "        # add config columns\n",
    "        players.insert(0, 'match_id', match_request['matchInfo']['id'])\n",
    "        players.insert(1, 'team_id', team.contestantId)\n",
    "        config_cols = ['match_id', 'team_id'] + cols_to_keep\n",
    "\n",
    "        # add all columns not found in player data to keep StatsPerform detailed player statistics schema \n",
    "        # (schema can be found in config.py)\n",
    "        stat_cols = players.drop(config_cols, axis=1).columns\n",
    "        missing_cols = set(config.all_player_match_stats).difference(stat_cols)\n",
    "\n",
    "        if missing_cols:\n",
    "            missing_df = pd.DataFrame(0, index=players.index, columns=list(missing_cols), dtype=float)\n",
    "            players = pd.concat([players, missing_df], axis=1)\n",
    "\n",
    "        list_of_dfs.append(players[config_cols + config.all_player_match_stats])\n",
    "\n",
    "    # return dataframe of all players with their statistics\n",
    "    return pd.concat(list_of_dfs, axis=0, ignore_index=True).fillna(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1458943",
   "metadata": {},
   "source": [
    "# feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dadd050",
   "metadata": {},
   "source": [
    "## data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedb491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_team_data():\n",
    "        query = f\"\"\"\n",
    "                select *\n",
    "                from prizepicksanalytics.soccer_simulations.team_match_data\n",
    "                \"\"\"\n",
    "        \n",
    "        return prem_loader.execute_bq_query(query)\n",
    "\n",
    "def get_all_player_data():\n",
    "        query = f\"\"\"\n",
    "                select *\n",
    "                from prizepicksanalytics.soccer_simulations.player_match_data\n",
    "                \"\"\"\n",
    "        \n",
    "        return prem_loader.execute_bq_query(query)\n",
    "\n",
    "def get_historic_team_data(match):\n",
    "\n",
    "        home_query = f\"\"\"\n",
    "                select *\n",
    "                from prizepicksanalytics.soccer_simulations.team_match_data\n",
    "                where team_id = '{match.home_id}'\n",
    "                and date(REPLACE(match_date, 'Z', '')) < date('{match.localDate}')\n",
    "                \"\"\"\n",
    "        \n",
    "        away_query = f\"\"\"\n",
    "                select *\n",
    "                from prizepicksanalytics.soccer_simulations.team_match_data\n",
    "                where team_id = '{match.away_id}'\n",
    "                and date(REPLACE(match_date, 'Z', '')) < date('{match.localDate}')\n",
    "                \"\"\"\n",
    "\n",
    "\n",
    "        return prem_loader.execute_bq_query(home_query),   prem_loader.execute_bq_query(away_query)\n",
    "\n",
    "\n",
    "# def get_historic_player_data(match,\n",
    "#                              list_of_player_ids):\n",
    "        \n",
    "#         player_query = f\"\"\"\n",
    "#                 select *\n",
    "#                 from prizepicksanalytics.soccer_simulations.player_match_data\n",
    "#                 where playerId in ({\", \".join([\"'\" + x + \"'\" for x in list_of_player_ids])})\n",
    "#                 and date(REPLACE(match_date, 'Z', '')) < date('{match.localDate}')\n",
    "#                 \"\"\"\n",
    "\n",
    "\n",
    "#         return prem_loader.execute_bq_query(player_query)\n",
    "\n",
    "        # return player_query\n",
    "\n",
    "def get_squads(match):\n",
    "\n",
    "        home_query = f\"\"\"\n",
    "                select *\n",
    "                from prizepicksanalytics.soccer_simulations.squad_data\n",
    "                where contestantId = '{match.home_id}'\n",
    "                \"\"\"\n",
    "        \n",
    "        away_query = f\"\"\"\n",
    "                select *\n",
    "                from prizepicksanalytics.soccer_simulations.squad_data\n",
    "                where contestantId = '{match.away_id}'\n",
    "                \"\"\"\n",
    "\n",
    "\n",
    "        return prem_loader.execute_bq_query(home_query), prem_loader.execute_bq_query(away_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12f93e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# away_query = f\"\"\"\n",
    "#                 select *\n",
    "#                 from prizepicksanalytics.soccer_simulations.squad_data\n",
    "#                 \"\"\"\n",
    "\n",
    "\n",
    "# squad_data = prem_loader.execute_bq_query(away_query)\n",
    "\n",
    "\n",
    "# tourneys = get_statsperform_tourneys()\n",
    "# team_data = get_all_team_data()\n",
    "# team_data.to_csv('team_data.csv', index=False)\n",
    "# player_data = get_all_player_data()\n",
    "# player_data.to_csv('player_data.csv', index=False)\n",
    "# ars_lfc_match = prem_loader.matches.iloc[290]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c45489",
   "metadata": {},
   "source": [
    "## agg methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4149850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_team_stats_vector(team_data,\n",
    "                           opposite_team_data,\n",
    "                           position):\n",
    "    \n",
    "    home = True if position == 'home' else False\n",
    "    position_performances = team_data[team_data['home'] == home]\n",
    "\n",
    "    if team_data.shape[0] == 0:\n",
    "        avg_possession = np.nan\n",
    "        avg_finalThirdEntries = np.nan\n",
    "        avg_accurateBackZonePass = np.nan\n",
    "        avg_accurateFwdZonePass = np.nan\n",
    "\n",
    "        avg_possWonAtt3rd = np.nan\n",
    "        avg_possWonMid3rd = np.nan\n",
    "        avg_interception = np.nan\n",
    "    else:\n",
    "        avg_possession = team_data.possessionPercentage.mean()\n",
    "        avg_finalThirdEntries = team_data.finalThirdEntries.mean()\n",
    "        avg_accurateBackZonePass = team_data.accurateBackZonePass.mean()\n",
    "        avg_accurateFwdZonePass = team_data.accurateFwdZonePass.mean()\n",
    "\n",
    "        avg_possWonAtt3rd = team_data.possWonAtt3rd.mean()\n",
    "        avg_possWonMid3rd = team_data.possWonMid3rd.mean()\n",
    "        avg_interception = team_data.interception.mean()\n",
    "\n",
    "    if position_performances.shape[0] == 0:\n",
    "        ps_possession = np.nan\n",
    "        ps_finalThirdEntries = np.nan\n",
    "        ps_accurateBackZonePass = np.nan\n",
    "        ps_accurateFwdZonePass = np.nan\n",
    "\n",
    "        ps_possWonAtt3rd = np.nan\n",
    "        ps_possWonMid3rd = np.nan\n",
    "        ps_interception = np.nan\n",
    "    else:\n",
    "        ps_possession = position_performances.possessionPercentage.mean()\n",
    "        ps_finalThirdEntries = position_performances.finalThirdEntries.mean()\n",
    "        ps_accurateBackZonePass = position_performances.accurateBackZonePass.mean()\n",
    "        ps_accurateFwdZonePass = position_performances.accurateFwdZonePass.mean()\n",
    "\n",
    "        ps_possWonAtt3rd = position_performances.possWonAtt3rd.mean()\n",
    "        ps_possWonMid3rd = position_performances.possWonMid3rd.mean()\n",
    "        ps_interception = position_performances.interception.mean()\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'avg_possession': avg_possession,\n",
    "        'avg_finalThirdEntries': avg_finalThirdEntries,\n",
    "        'avg_accurateBackZonePass': avg_accurateBackZonePass,\n",
    "        'avg_accurateFwdZonePass': avg_accurateFwdZonePass,\n",
    "        'avg_possWonAtt3rd': avg_possWonAtt3rd,\n",
    "        'avg_possWonMid3rd': avg_possWonMid3rd,\n",
    "        'avg_interception': avg_interception,\n",
    "\n",
    "        'ps_possession': ps_possession,\n",
    "        'ps_finalThirdEntries': ps_finalThirdEntries,\n",
    "        'ps_accurateBackZonePass': ps_accurateBackZonePass,\n",
    "        'ps_accurateFwdZonePass': ps_accurateFwdZonePass,\n",
    "        'ps_possWonAtt3rd': ps_possWonAtt3rd,\n",
    "        'ps_possWonMid3rd': ps_possWonMid3rd,\n",
    "        'ps_interception': ps_interception,\n",
    "    }\n",
    "\n",
    "def _get_player_stats(player_performances,\n",
    "                      position):\n",
    "    home = True if position == 'home' else False\n",
    "    position_performances = player_performances[player_performances['home'] == home]\n",
    "\n",
    "    if team_data.shape[0] == 0:\n",
    "        avg_passAtt = np.nan\n",
    "        avg_minsPlayed = np.nan\n",
    "        # avg_finalThirdEntries = np.nan\n",
    "        # avg_accurateBackZonePass = np.nan\n",
    "        # avg_accurateFwdZonePass = np.nan\n",
    "\n",
    "        # avg_possWonAtt3rd = np.nan\n",
    "        # avg_possWonMid3rd = np.nan\n",
    "        # avg_interception = np.nan\n",
    "    else:\n",
    "        avg_passAtt = player_performances.totalPass.mean()\n",
    "        avg_minsPlayed = player_performances.minsPlayed.mean()\n",
    "        # avg_finalThirdEntries = team_data.finalThirdEntries.mean()\n",
    "        # avg_accurateBackZonePass = team_data.accurateBackZonePass.mean()\n",
    "        # avg_accurateFwdZonePass = team_data.accurateFwdZonePass.mean()\n",
    "\n",
    "        # avg_possWonAtt3rd = team_data.possWonAtt3rd.mean()\n",
    "        # avg_possWonMid3rd = team_data.possWonMid3rd.mean()\n",
    "        # avg_interception = team_data.interception.mean()\n",
    "\n",
    "    if position_performances.shape[0] == 0:\n",
    "        ps_passAtt = np.nan\n",
    "        ps_minsPlayed = np.nan\n",
    "        # ps_finalThirdEntries = np.nan\n",
    "        # ps_accurateBackZonePass = np.nan\n",
    "        # ps_accurateFwdZonePass = np.nan\n",
    "\n",
    "        # ps_possWonAtt3rd = np.nan\n",
    "        # ps_possWonMid3rd = np.nan\n",
    "        # ps_interception = np.nan\n",
    "    else:\n",
    "        ps_passAtt = position_performances.totalPass.mean()\n",
    "        ps_minsPlayed = position_performances.minsPlayed.mean()\n",
    "        # ps_finalThirdEntries = position_performances.finalThirdEntries.mean()\n",
    "        # ps_accurateBackZonePass = position_performances.accurateBackZonePass.mean()\n",
    "        # ps_accurateFwdZonePass = position_performances.accurateFwdZonePass.mean()\n",
    "\n",
    "        # ps_possWonAtt3rd = position_performances.possWonAtt3rd.mean()\n",
    "        # ps_possWonMid3rd = position_performances.possWonMid3rd.mean()\n",
    "        # ps_interception = position_performances.interception.mean()\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        # 'player_id':player_performances.matchName.iloc[0],\n",
    "        'avg_passes': avg_passAtt,\n",
    "        'avg_minsPlayed': avg_minsPlayed,\n",
    "        # 'avg_accurateBackZonePass': avg_accurateBackZonePass,\n",
    "        # 'avg_accurateFwdZonePass': avg_accurateFwdZonePass,\n",
    "        # 'avg_possWonAtt3rd': avg_possWonAtt3rd,\n",
    "        # 'avg_possWonMid3rd': avg_possWonMid3rd,\n",
    "        # 'avg_interception': avg_interception,\n",
    "\n",
    "        'ps_passes': ps_passAtt,\n",
    "        'ps_minsPlayed': ps_minsPlayed,\n",
    "        # 'ps_accurateBackZonePass': ps_accurateBackZonePass,\n",
    "        # 'ps_accurateFwdZonePass': ps_accurateFwdZonePass,\n",
    "        # 'ps_possWonAtt3rd': ps_possWonAtt3rd,\n",
    "        # 'ps_possWonMid3rd': ps_possWonMid3rd,\n",
    "        # 'ps_interception': ps_interception,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3236050",
   "metadata": {},
   "source": [
    "## get match vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403d327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "    \n",
    "def get_match_vectors(match,\n",
    "                      all_performances = None,\n",
    "                      all_squads=None,\n",
    "                      all_player_data=None,\n",
    "                      training=True):\n",
    "\n",
    "    final_feat_list = []\n",
    "\n",
    "    team_ids = {'home':match.home_id,\n",
    "                'away':match.away_id}\n",
    "\n",
    "    if all_performances is None:\n",
    "        home_data, away_data = get_historic_team_data(match)\n",
    "    else:\n",
    "        home_data = all_performances[(all_performances['team_id'] == team_ids['home']) &\n",
    "                                     (all_performances['match_date'].apply(lambda x: x.replace('Z', '')) < match.localDate)]\n",
    "        \n",
    "        away_data = all_performances[(all_performances['team_id'] == team_ids['away']) &\n",
    "                                     (all_performances['match_date'].apply(lambda x: x.replace('Z', '')) < match.localDate)]\n",
    "\n",
    "    team_data = {\n",
    "        'home':home_data,\n",
    "        'away':away_data\n",
    "    }\n",
    "\n",
    "    if all_squads is None:\n",
    "        home_squad, away_squad = get_squads(match)\n",
    "    else:\n",
    "        home_squad, away_squad = all_squads[team_ids['home']], all_squads[team_ids['away']]\n",
    "\n",
    "    squad_data = {\n",
    "        'home':home_squad,\n",
    "        'away':away_squad\n",
    "    }\n",
    "\n",
    "    for team in ['home', 'away']:\n",
    "        opposite_position = 'away' if team == 'home' else 'home'\n",
    "\n",
    "        team_stats_vector = _get_team_stats_vector(team_data[team],\n",
    "                                                      team_data[opposite_position],\n",
    "                                                      team)\n",
    "        \n",
    "        team_squad = squad_data[team]\n",
    "\n",
    "        if all_player_data is None:\n",
    "            historical_player_data = get_historic_player_data(match, list(team_squad.id.unique()))\n",
    "        else: \n",
    "            historical_player_data = all_player_data[(all_player_data['playerId'].isin(team_squad.id.unique())) &\n",
    "                                                      (all_player_data['match_date'].apply(lambda x: x.replace('Z', '')) < match.localDate)]\n",
    "            \n",
    "        # team_squad = team_squad[team_squad['id'].isin(historical_player_data.playerId.unique())]\n",
    "        team_squad = all_player_data[(all_player_data['team_id'] == team_ids[team]) & (all_player_data['match_id'] == match.id)]\n",
    "\n",
    "        for i, player in team_squad.iterrows():\n",
    "\n",
    "            final_dict = {\n",
    "                'player_id':player.playerId,\n",
    "                'player_name':player.matchName,\n",
    "                'player_position':player.position,\n",
    "                'match_id':match.id,\n",
    "                'match_date':match.localDate,\n",
    "                'match_week':match.week\n",
    "            }\n",
    "\n",
    "            # print('player id', player.playerId)\n",
    "\n",
    "            player_performances = historical_player_data[historical_player_data['playerId'] == player.playerId]\n",
    "            player_vector = _get_player_stats(player_performances,\n",
    "                                              team)\n",
    "            \n",
    "            final_dict.update(player_vector)\n",
    "            final_dict.update(team_stats_vector)          \n",
    "\n",
    "            if training == True:\n",
    "                try:\n",
    "                    mask = (all_player_data['match_id'] == match.id) & (all_player_data['playerId'] == player.playerId)\n",
    "                    final_dict['target'] = all_player_data[mask].iloc[0].totalPass\n",
    "                except IndexError as e:\n",
    "                    continue\n",
    "        \n",
    "            # player_vector['match_id'] = match.id\n",
    "            # player_vector['match_date'] = match.localDate\n",
    "            # player_vector['match_week'] = match.week\n",
    "\n",
    "            final_feat_list.append(final_dict)\n",
    "        \n",
    "    return final_feat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b81ca8",
   "metadata": {},
   "source": [
    "## feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d675fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "team_data = pd.read_csv('team_data.csv')\n",
    "team_data = team_data.sort_values(['match_date'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "player_data = pd.read_csv('player_data.csv')\n",
    "player_data = player_data.sort_values(['match_date', 'match_id', 'team_id', 'playerId'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "with open('squads.pickle', 'rb') as fila:\n",
    "    all_squads = pickle.load(fila)\n",
    "\n",
    "\n",
    "\n",
    "list_of_all_vectors = []\n",
    "\n",
    "for i, match in prem_loader.matches.iterrows():\n",
    "\n",
    "    vectors = get_match_vectors(match,\n",
    "                    all_performances=team_data,\n",
    "                    all_squads=all_squads,\n",
    "                    all_player_data=player_data,\n",
    "                    training=True)\n",
    "                    \n",
    "\n",
    "    list_of_all_vectors.extend(vectors)\n",
    "\n",
    "feature_list = pd.DataFrame(list_of_all_vectors)\n",
    "feature_list.to_csv('./v1features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b417e9b3",
   "metadata": {},
   "source": [
    "# modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83577423",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('v1features.csv', index_col=0)\n",
    "features = features.dropna()\n",
    "features = features.sort_values('match_date', ascending=True)\n",
    "\n",
    "weeks = features.match_week.unique()\n",
    "test_split = int(len(weeks) * 0.8)\n",
    "\n",
    "train_weeks = weeks[:test_split]\n",
    "test_weeks = weeks[test_split:]\n",
    "\n",
    "train = features[features['match_week'].isin(train_weeks)]\n",
    "test = features[features['match_week'].isin(test_weeks)]\n",
    "\n",
    "\n",
    "config_cols = ['player_name', 'match_id', 'match_date', 'match_week', 'player_id']\n",
    "feats = train.drop(columns=config_cols + ['target']).columns\n",
    "target = ['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd94f3b8",
   "metadata": {},
   "source": [
    "## light gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d31950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.lightgbm.lgb import LGBMOrdinal\n",
    "\n",
    "# model = LGBMOrdinal(num_leaves=24, max_depth=2, n_estimators=1500, learning_rate=0.001)\n",
    "\n",
    "# model.fit(train[feats], train[target])\n",
    "# probs = model.predict_proba(test[feats])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd5203d",
   "metadata": {},
   "source": [
    "## alexboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9ad425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alexboost import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd54fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Normal(\n",
    "        num_trees=50, \n",
    "        num_samples_per_bucket=30,\n",
    "    )\n",
    "\n",
    "\n",
    "norm.fit(train[feats], train[target].target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28409fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = norm.predict(test[feats])\n",
    "test[['exp_mean', 'exp_std']] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a38f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns # Often imported as sns\n",
    "\n",
    "# test.exp_std.plot()\n",
    "sns.scatterplot(\n",
    "    data=test,\n",
    "    x='exp_std',\n",
    "    y='exp_std',\n",
    "    hue=''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe3328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm.regressor.feature_importances_\n",
    "pd.DataFrame({\"feature\": train[feats].columns, \"importance\": norm.regressor.feature_importances_}).sort_values(by=\"importance\", ascending=False).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f72095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train[target].target.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6250e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.sort_values('target', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf1c248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

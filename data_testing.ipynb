{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e0711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc7a08d",
   "metadata": {},
   "source": [
    "# statsperform API + bq backloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa939eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import data\n",
    "importlib.reload(data)\n",
    "from data import DataLoader\n",
    "\n",
    "prem2425 = '9n12waklv005j8r32sfjj2eqc'\n",
    "\n",
    "prem_loader = DataLoader(prem2425)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1458943",
   "metadata": {},
   "source": [
    "# feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dadd050",
   "metadata": {},
   "source": [
    "## data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedb491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_team_data():\n",
    "        query = f\"\"\"\n",
    "                select *\n",
    "                from prizepicksanalytics.soccer_simulations.team_match_data\n",
    "                \"\"\"\n",
    "        \n",
    "        return prem_loader.execute_bq_query(query)\n",
    "\n",
    "def get_all_player_data():\n",
    "        query = f\"\"\"\n",
    "                select *\n",
    "                from prizepicksanalytics.soccer_simulations.player_match_data\n",
    "                \"\"\"\n",
    "        \n",
    "        return prem_loader.execute_bq_query(query)\n",
    "\n",
    "def get_historic_team_data(match):\n",
    "\n",
    "        home_query = f\"\"\"\n",
    "                select *\n",
    "                from prizepicksanalytics.soccer_simulations.team_match_data\n",
    "                where team_id = '{match.home_id}'\n",
    "                and date(REPLACE(match_date, 'Z', '')) < date('{match.localDate}')\n",
    "                \"\"\"\n",
    "        \n",
    "        away_query = f\"\"\"\n",
    "                select *\n",
    "                from prizepicksanalytics.soccer_simulations.team_match_data\n",
    "                where team_id = '{match.away_id}'\n",
    "                and date(REPLACE(match_date, 'Z', '')) < date('{match.localDate}')\n",
    "                \"\"\"\n",
    "\n",
    "\n",
    "        return prem_loader.execute_bq_query(home_query),   prem_loader.execute_bq_query(away_query)\n",
    "\n",
    "\n",
    "# def get_historic_player_data(match,\n",
    "#                              list_of_player_ids):\n",
    "        \n",
    "#         player_query = f\"\"\"\n",
    "#                 select *\n",
    "#                 from prizepicksanalytics.soccer_simulations.player_match_data\n",
    "#                 where playerId in ({\", \".join([\"'\" + x + \"'\" for x in list_of_player_ids])})\n",
    "#                 and date(REPLACE(match_date, 'Z', '')) < date('{match.localDate}')\n",
    "#                 \"\"\"\n",
    "\n",
    "\n",
    "#         return prem_loader.execute_bq_query(player_query)\n",
    "\n",
    "        # return player_query\n",
    "\n",
    "def get_squads(match):\n",
    "\n",
    "        home_query = f\"\"\"\n",
    "                select *\n",
    "                from prizepicksanalytics.soccer_simulations.squad_data\n",
    "                where contestantId = '{match.home_id}'\n",
    "                \"\"\"\n",
    "        \n",
    "        away_query = f\"\"\"\n",
    "                select *\n",
    "                from prizepicksanalytics.soccer_simulations.squad_data\n",
    "                where contestantId = '{match.away_id}'\n",
    "                \"\"\"\n",
    "\n",
    "\n",
    "        return prem_loader.execute_bq_query(home_query), prem_loader.execute_bq_query(away_query)\n",
    "\n",
    "\n",
    "def get_calendar():\n",
    "    query = \"\"\"\n",
    "        select * from prizepicksanalytics.soccer_simulations.match_calendar\n",
    "    \"\"\"\n",
    "    return prem_loader.execute_bq_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12f93e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# away_query = f\"\"\"\n",
    "#                 select *\n",
    "#                 from prizepicksanalytics.soccer_simulations.squad_data\n",
    "#                 \"\"\"\n",
    "\n",
    "\n",
    "# squad_data = prem_loader.execute_bq_query(away_query)\n",
    "\n",
    "# calendar_data = get_calendar()\n",
    "# calendar_data.to_csv('calendar_data.csv', index=False)\n",
    "\n",
    "\n",
    "# tourneys = get_statsperform_tourneys()\n",
    "# team_data = get_all_team_data()\n",
    "# team_data.to_csv('team_data.csv', index=False)\n",
    "# player_data = get_all_player_data()\n",
    "# player_data.to_csv('player_data.csv', index=False)\n",
    "# ars_lfc_match = prem_loader.matches.iloc[290]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c45489",
   "metadata": {},
   "source": [
    "## agg methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4149850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_team_stats_vector(team_data,\n",
    "                           opposite_team_data,\n",
    "                           position):\n",
    "    \n",
    "    home = True if position == 'home' else False\n",
    "\n",
    "    if team_data.shape[0] == 0:\n",
    "        avg_possession = np.nan\n",
    "        weighted_possession = np.nan\n",
    "    else:\n",
    "        avg_possession = team_data.possessionPercentage.mean()\n",
    "        weighted_possession = (team_data.possessionPercentage * team_data.poss_weight).sum() / team_data.poss_weight.sum()\n",
    "\n",
    "\n",
    "    \n",
    "    return {\n",
    "        't_avg_possession': avg_possession,\n",
    "        \"t_posw_possession\": weighted_possession\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def _get_player_stats(player_performances,\n",
    "                      team_performances,\n",
    "                      position):\n",
    "    home = True if position == 'home' else False\n",
    "    \n",
    "    team_performances = team_performances.sort_values('match_date', ascending=True)\n",
    "    last_5_team_perf = team_performances.iloc[-5:].match_id\n",
    "\n",
    "    player_last_5_perf = player_performances[player_performances['match_id'].isin(last_5_team_perf)]\n",
    "\n",
    "    if player_performances.shape[0] == 0:\n",
    "        avg_passAtt = np.nan\n",
    "        std_passAtt = np.nan\n",
    "        avg_minsPlayed = np.nan\n",
    "        # last5_mins = np.nan\n",
    "        weighted_passAtt = np.nan\n",
    "\n",
    "    else:\n",
    "        avg_passAtt = player_performances.totalPass.mean()\n",
    "        std_passAtt = player_performances.totalPass.std()\n",
    "        avg_minsPlayed = player_performances.minsPlayed.mean()\n",
    "        # last5_mins = player_performances\n",
    "        weighted_passAtt = (player_performances['totalPass'] * player_performances['poss_weight'].astype(float)).sum() / player_performances['poss_weight'].astype(float).sum()\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'p_avg_passes': avg_passAtt,\n",
    "        'p_std_passes': std_passAtt,\n",
    "        'p_avg_minsPlayed': avg_minsPlayed,\n",
    "        'p_weighted_passes': weighted_passAtt,\n",
    "        'p_last5_minsPlayed': player_last_5_perf.minsPlayed.mean() if not player_last_5_perf.empty else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4715c46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_mappings = {\n",
    "  \"b9si1jn1lfxfund69e9ogcu2n\": \"Wolves\",\n",
    "  \"7yx5dqhhphyvfisohikodajhv\": \"Brentford\",\n",
    "  \"22doj4sgsocqpxw45h607udje\": \"Tottenham\",\n",
    "  \"e5p0ehyguld7egzhiedpdnc3w\": \"Brighton\",\n",
    "  \"d5ydtvt96bv7fq04yqm2w2632\": \"Southampton\",\n",
    "  \"4dsgumo7d4zupm2ugsvm4zm4d\": \"Arsenal\",\n",
    "  \"1qtaiy11gswx327s0vkibf70n\": \"Nottm Forest\",\n",
    "  \"9q0arba2kbnywth8bkxlhgmdr\": \"Chelsea\",\n",
    "  \"7vn2i2kd35zuetw6b38gw9jsz\": \"Newcastle\",\n",
    "  \"ehd2iemqmschhj2ec0vayztzz\": \"Everton\",\n",
    "  \"6eqit8ye8aomdsrrq0hk3v7gh\": \"Man Utd\",\n",
    "  \"b496gs285it6bheuikox6z9mj\": \"Aston Villa\",\n",
    "  \"c8h9bw1l82s06h77xxrelzhur\": \"Liverpool\",\n",
    "  \"1c8m2ko0wxq1asfkuykurdr0y\": \"Crystal Palace\",\n",
    "  \"8b523ujgl21tbc01me65q0aoh\": \"Ipswich\",\n",
    "  \"4txjdaqveermfryvbfrr4taf7\": \"West Ham\",\n",
    "  \"hzqh7z0mdl3v7gwete66syxp\": \"Fulham\",\n",
    "  \"a3nyxabgsqlnqfkeg41m6tnpp\": \"Man City\",\n",
    "  \"1pse9ta7a45pi2w2grjim70ge\": \"Bournemouth\",\n",
    "  \"avxknfz4f6ob0rv9dbnxdzde0\": \"Leicester\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3236050",
   "metadata": {},
   "source": [
    "## get match vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403d327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gaussian_weight(historic_possession, predicted_next_possession, bandwidth=20.0):\n",
    "    \"\"\"\n",
    "    Calculates a Gaussian weight based on the similarity of possession percentages.\n",
    "    Bandwidth (sigma) controls how quickly the weight drops off.\n",
    "    \"\"\"\n",
    "    difference = historic_possession - predicted_next_possession\n",
    "    # Larger bandwidth means less sensitive to differences\n",
    "    return np.exp(-(difference**2) / (2 * bandwidth**2))\n",
    "\n",
    "    \n",
    "def get_match_vectors(match,\n",
    "                      all_performances = None,\n",
    "                      all_squads=None,\n",
    "                      all_player_data=None,\n",
    "                      training=True):\n",
    "\n",
    "    final_feat_list = []\n",
    "\n",
    "    team_ids = {'home':match.home_id,\n",
    "                'away':match.away_id}\n",
    "\n",
    "    if all_performances is None:\n",
    "        home_data, away_data = get_historic_team_data(match)\n",
    "    else:\n",
    "        home_data = all_performances[(all_performances['team_id'] == team_ids['home']) &\n",
    "                                     (all_performances['match_date'].apply(lambda x: x.replace('Z', '')) < match.localDate)]\n",
    "        \n",
    "        away_data = all_performances[(all_performances['team_id'] == team_ids['away']) &\n",
    "                                     (all_performances['match_date'].apply(lambda x: x.replace('Z', '')) < match.localDate)]\n",
    "\n",
    "    team_data = {\n",
    "        'home':home_data,\n",
    "        'away':away_data\n",
    "    }\n",
    "\n",
    "    if all_squads is None:\n",
    "        home_squad, away_squad = get_squads(match)\n",
    "    else:\n",
    "        home_squad, away_squad = all_squads[team_ids['home']], all_squads[team_ids['away']]\n",
    "\n",
    "    squad_data = {\n",
    "        'home':home_squad,\n",
    "        'away':away_squad\n",
    "    }\n",
    "\n",
    "    # print(match.description)\n",
    "    for team in ['home', 'away']:\n",
    "        # print(team_mappings[team_ids[team]])\n",
    "\n",
    "        mask2 = (all_performances['match_id'] == match.id) & (all_performances['team_id'] == team_ids[team])\n",
    "        # USED FOR GAUSSIAN KERNEL\n",
    "        cheat_possession = all_performances[mask2].iloc[0].possessionPercentage\n",
    "\n",
    "        # GAUSSIAN KERNEL APPLICATION\n",
    "        team_data[team]['poss_weight'] = gaussian_weight(team_data[team].possessionPercentage, \n",
    "                                                         cheat_possession,\n",
    "                                                         team_data[team].possessionPercentage.std())\n",
    "        \n",
    "        team_data[team]['poss_weight'] = team_data[team]['poss_weight'].astype(float)**2\n",
    "        \n",
    "        team_data[team]['opponent_id'] = team_data[team]['opponent_id'].apply(lambda x: team_mappings[x])\n",
    "        # print(team_data[team][['opponent_id', 'possessionPercentage', 'poss_weight']].sort_values('poss_weight', ascending=False).reset_index(drop=True))\n",
    "        \n",
    "\n",
    "        opposite_position = 'away' if team == 'home' else 'home'\n",
    "\n",
    "        team_stats_vector = _get_team_stats_vector(team_data[team],\n",
    "                                                      team_data[opposite_position],\n",
    "                                                      team)\n",
    "        \n",
    "        \n",
    "        team_squad = squad_data[team]\n",
    "\n",
    "        if all_player_data is None:\n",
    "            historical_player_data = get_historic_player_data(match, list(team_squad.id.unique()))\n",
    "        else: \n",
    "            historical_player_data = all_player_data[(all_player_data['playerId'].isin(team_squad.id.unique())) &\n",
    "                                                      (all_player_data['match_date'].apply(lambda x: x.replace('Z', '')) < match.localDate)]\n",
    "            \n",
    "        # merge match weights on historical player data\n",
    "        historical_player_data = historical_player_data.merge(team_data[team][['match_id', 'team_id', 'poss_weight']],\n",
    "                                     how='left',\n",
    "                                     on=['match_id', 'team_id'])\n",
    "        \n",
    "            \n",
    "        # team_squad = team_squad[team_squad['id'].isin(historical_player_data.playerId.unique())]\n",
    "        team_squad = all_player_data[(all_player_data['team_id'] == team_ids[team]) & (all_player_data['match_id'] == match.id)]\n",
    "\n",
    "        for i, player in team_squad.iterrows():\n",
    "\n",
    "            final_dict = {\n",
    "                'player_id':player.playerId,\n",
    "                'player_name':player.matchName,\n",
    "                'player_position':player.position,\n",
    "                'team_id':team_ids[team],\n",
    "                'match_id':match.id,\n",
    "                'match_date':match.localDate,\n",
    "                'match_week':match.week,\n",
    "                'is_home':True if team == 'home' else False\n",
    "                # 'cheating_possession':\n",
    "            }\n",
    "\n",
    "            # print('player id', player.playerId)\n",
    "\n",
    "            player_performances = historical_player_data[historical_player_data['playerId'] == player.playerId]\n",
    "\n",
    "\n",
    "            player_vector = _get_player_stats(player_performances,\n",
    "                                              team_data[team],\n",
    "                                              team)\n",
    "            \n",
    "            \n",
    "            \n",
    "            final_dict.update(player_vector)\n",
    "            final_dict.update(team_stats_vector)      \n",
    "\n",
    "            # print(final_dict['p_weighted_passes'])    \n",
    "\n",
    "            if training == True:\n",
    "                try:\n",
    "                    mask = (all_player_data['match_id'] == match.id) & (all_player_data['playerId'] == player.playerId)\n",
    "                    performance = all_player_data[mask].iloc[0]\n",
    "\n",
    "                    final_dict['is_sub'] = performance.isSub\n",
    "                    final_dict['target'] = performance.totalPass\n",
    "\n",
    "                except IndexError as e:\n",
    "                    continue\n",
    "        \n",
    "            # player_vector['match_id'] = match.id\n",
    "            # player_vector['match_date'] = match.localDate\n",
    "            # player_vector['match_week'] = match.week\n",
    "\n",
    "            final_feat_list.append(final_dict)\n",
    "        \n",
    "    return final_feat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b81ca8",
   "metadata": {},
   "source": [
    "## feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d675fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "calendar = pd.read_csv('match_calendar.csv')\n",
    "\n",
    "team_data = pd.read_csv('team_data.csv')\n",
    "team_data = team_data.sort_values(['match_date'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "player_data = pd.read_csv('player_data.csv')\n",
    "player_data = player_data.sort_values(['match_date', 'match_id', 'team_id', 'playerId'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "with open('squads.pickle', 'rb') as fila:\n",
    "    all_squads = pickle.load(fila)\n",
    "\n",
    "list_of_all_vectors = []\n",
    "\n",
    "for i, match in calendar.iterrows():\n",
    "\n",
    "    vectors = get_match_vectors(match,\n",
    "                    all_performances=team_data,\n",
    "                    all_squads=all_squads,\n",
    "                    all_player_data=player_data,\n",
    "                    training=True)\n",
    "                    \n",
    "\n",
    "    list_of_all_vectors.extend(vectors)\n",
    "\n",
    "feature_list = pd.DataFrame(list_of_all_vectors)\n",
    "feature_list.to_csv('./vcheatfeatures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13119293",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b417e9b3",
   "metadata": {},
   "source": [
    "# modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83577423",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'cheat'\n",
    "\n",
    "features = pd.read_csv(f'v{version}features.csv', index_col=0)\n",
    "features = features.dropna()\n",
    "features = features.sort_values('match_date', ascending=True)\n",
    "\n",
    "if version != 2:\n",
    "    features = features.drop(index=features[features['player_position'] == 'Substitute'].index)\n",
    "    position_dummies = pd.get_dummies(features.player_position).astype(int)\n",
    "    features = features.merge(position_dummies, left_index=True, right_index=True, how='left')\n",
    "\n",
    "weeks = features.match_week.unique()\n",
    "test_split = int(len(weeks) * 0.8)\n",
    "\n",
    "train_weeks = weeks[:test_split]\n",
    "test_weeks = weeks[test_split:]\n",
    "\n",
    "train = features[features['match_week'].isin(train_weeks)]\n",
    "test = features[features['match_week'].isin(test_weeks)]\n",
    "\n",
    "config_cols = ['player_name', 'player_position', 'team_id', 'match_id', 'match_date', 'match_week', 'player_id', 'p_std_passes']\n",
    "feats = train.drop(columns=config_cols + ['target']).columns\n",
    "target = ['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd5203d",
   "metadata": {},
   "source": [
    "## alexboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9ad425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alexboost import NegBinomial\n",
    "from alexboost import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd54fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "alx_norm = Normal(\n",
    "        num_trees=50, \n",
    "        num_samples_per_bucket=30,\n",
    "    )\n",
    "\n",
    "alx_neg_bin = NegBinomial(\n",
    "        num_trees=50, \n",
    "        num_samples_per_bucket=30,\n",
    "    )\n",
    "\n",
    "\n",
    "# alx_norm.fit(train[feats], train[target].target)\n",
    "alx_neg_bin.fit(train[feats], train[target].target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28409fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm_preds = alx_norm.predict(test[feats])\n",
    "neg_bin_preds = alx_neg_bin.predict(test[feats])\n",
    "\n",
    "# test[['norm_mean', 'norm_std']] = norm_preds\n",
    "test[['nb_mean', 'nb_r']] = neg_bin_preds\n",
    "\n",
    "test['error'] = (test.target - test.nb_mean)\n",
    "# test['avg_error'] = (test.target - test.avg_passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc587fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.team_id = test.team_id.apply(lambda x: team_mappings[x])\n",
    "pd.DataFrame(test.groupby(['team_id', 'player_name'])['error'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eb66da",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4abcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm, nbinom # Import the distributions\n",
    "\n",
    "def normal_nll(y_true, mu, sigma):\n",
    "    log_prob = norm.logpdf(y_true, loc=mu, scale=sigma)\n",
    "    return -log_prob\n",
    "\n",
    "def negative_binomial_nll(y_true, mu, r):\n",
    "    p = r / (mu + r)\n",
    "    p = np.clip(p, 1e-16, 1.0 - 1e-16)\n",
    "\n",
    "    y_true = int(y_true)\n",
    "    log_prob = nbinom.logpmf(y_true, n=r, p=p)\n",
    "    return -log_prob\n",
    "\n",
    "# test['norm_nll'] = test.apply(lambda x: normal_nll(x.target, x.norm_mean, x.norm_std), axis=1)\n",
    "test['nb_nll'] = test.apply(lambda x: negative_binomial_nll(x.target, x.nb_mean, x.nb_r), axis=1)\n",
    "test['avg_nll'] = test.apply(lambda x: normal_nll(x.target, x.p_avg_passes, x.p_std_passes), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07016427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm_mae = np.sum((np.abs(test.target - test.norm_mean)) / test.shape[0])\n",
    "negbin_mae = np.sum((np.abs(test.target - test.nb_mean)) / test.shape[0])\n",
    "avg_mae = np.sum((np.abs(test.target - test.p_avg_passes)) / test.shape[0])\n",
    "\n",
    "# norm_rmse = np.sqrt(np.sum(np.abs(test.target - test.norm_mean)**2 / test.shape[0]))\n",
    "negbin_rmse = np.sqrt(np.sum(np.abs(test.target - test.nb_mean)**2 / test.shape[0]))\n",
    "avg_rmse = np.sqrt(np.sum(np.abs(test.target - test.p_avg_passes)**2 / test.shape[0]))\n",
    "\n",
    "# print(f\"AlexBoost Norm Dist NLL: {test.norm_nll.mean()}\")\n",
    "# print(f\"AlexBoost NegBinom Dist \")\n",
    "# print(f\"Average Passes Norm Dist \")\n",
    "# print(f\"Norm MAE: {norm_mae} | Norm RMSE: {norm_rmse}\")\n",
    "print(f\"AlexBoost NegBinom -> MAE: {negbin_mae} | RMSE: {negbin_rmse} | NLL: {test.nb_nll.mean()}\")\n",
    "print(f\"Average Passes -> MAE: {avg_mae} | RMSE: {avg_rmse} | NLL: {test.avg_nll.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe881f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alx_norm.regressor.feature_importances_\n",
    "# pd.DataFrame({\"feature\": train[feats].columns, \"importance\": alx_norm.regressor.feature_importances_}).sort_values(by=\"importance\", ascending=False).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3726dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alx_neg_bin.regressor.feature_importances_\n",
    "pd.DataFrame({\"feature\": train[feats].columns, \"importance\": alx_neg_bin.regressor.feature_importances_}).sort_values(by=\"importance\", ascending=False).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b261ced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.sort_values('error', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc87e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "test['pred_mean_bin'] = pd.cut(test['nb_mean'], bins=20)\n",
    "\n",
    "calibration_data = test.groupby('pred_mean_bin').agg(\n",
    "    observed_mean=('target', 'mean'),\n",
    "    predicted_mean_avg=('nb_mean', 'mean'),\n",
    "    count=('target', 'size') # helpful to see how many samples in each bin\n",
    ").dropna()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=calibration_data['predicted_mean_avg'],\n",
    "    y=calibration_data['observed_mean'],\n",
    "    s=calibration_data['count'] * 2, # Scale point size by count in bin\n",
    "    color='blue',\n",
    "    alpha=0.7,\n",
    "    label='Binned Data (Size proportional to count)'\n",
    ")\n",
    "\n",
    "# Plot the ideal calibration line (y = x)\n",
    "max_val = max(calibration_data['predicted_mean_avg'].max(), calibration_data['observed_mean'].max())\n",
    "min_val = min(calibration_data['predicted_mean_avg'].min(), calibration_data['observed_mean'].min())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'k--', label='Perfectly Calibrated')\n",
    "\n",
    "plt.xlabel(\"Average Predicted Mean\")\n",
    "plt.ylabel(\"Average Actual Value\")\n",
    "plt.title(\"Calibration Curve: Average Actual vs. Average Predicted Mean\")\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# true_prob, pred_prob = calibration_curve(test['target'], test['nb_mean'], n_bins=10)\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.plot(pred_prob, true_prob, marker='o', label='Calibration Plot')\n",
    "# plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly calibrated')\n",
    "# plt.xlabel('Predicted \"Mean\" (conceptually probability)')\n",
    "# plt.ylabel('Actual \"Mean\" (conceptually frequency)')\n",
    "# plt.title('Scikit-learn style Calibration Curve (Conceptual)')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2636c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "test['target_bin'] = pd.qcut(test.target, q=15)\n",
    "\n",
    "sums_per_bin = test.groupby('target_bin')['error'].mean()\n",
    "\n",
    "bin_labels = [str(int(round(interval.left, 1))) + '-' + str(int(round(interval.right, 1))) for interval in sums_per_bin.index] # Convert intervals to strings for plotting\n",
    "bin_values = sums_per_bin.values\n",
    "\n",
    "plt.figure(figsize=(10, 6)) # Adjust figure size as needed\n",
    "plt.bar(bin_labels, bin_values, color='mediumseagreen') #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118ecb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['min_bin'] = pd.qcut(test.p_avg_minsPlayed, q=15)\n",
    "\n",
    "sums_per_bin = test.groupby('min_bin')['error'].mean()\n",
    "\n",
    "bin_labels = [str(int(round(interval.left, 1))) + '-' + str(int(round(interval.right, 1))) for interval in sums_per_bin.index] # Convert intervals to strings for plotting\n",
    "bin_values = sums_per_bin.values\n",
    "\n",
    "plt.figure(figsize=(10, 6)) # Adjust figure size as needed\n",
    "plt.bar(bin_labels, bin_values, color='mediumseagreen') #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['p_last5_minsPlayed_bin'] = pd.qcut(test.p_last5_minsPlayed, q=5)\n",
    "\n",
    "sums_per_bin = test.groupby('p_last5_minsPlayed_bin')['error'].mean()\n",
    "\n",
    "bin_labels = [str(int(round(interval.left, 1))) + '-' + str(int(round(interval.right, 1))) for interval in sums_per_bin.index] # Convert intervals to strings for plotting\n",
    "bin_values = sums_per_bin.values\n",
    "\n",
    "plt.figure(figsize=(10, 6)) # Adjust figure size as needed\n",
    "plt.bar(bin_labels, bin_values, color='mediumseagreen') #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579391c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee4bb3e",
   "metadata": {},
   "source": [
    "## prizepicks lines eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d93eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery as bqc\n",
    "\n",
    "prizepicks_lines_query = \"\"\" \n",
    "SELECT\n",
    "    start_time,\n",
    "    created_at,\n",
    "    game_id,\n",
    "    stat_type_name,\n",
    "    player_name,\n",
    "    line_score,\n",
    "    -- probability_of_more,\n",
    "    score,\n",
    "    description\n",
    "FROM (\n",
    "    SELECT\n",
    "        start_time,\n",
    "        created_at,\n",
    "        game_id,\n",
    "        stat_type_name,\n",
    "        player_name,\n",
    "        line_score,\n",
    "        -- COALESCE(probability_of_more, 0.5) AS probability_of_more,\n",
    "        -- probability_of_more,\n",
    "        score,\n",
    "        description,\n",
    "        ROW_NUMBER() OVER(PARTITION BY date(start_time_est), stat_type_name, player_name ORDER BY created_at ASC) as rn\n",
    "    FROM\n",
    "        `pick_level.pick_level`\n",
    "      WHERE stat_type_name = 'Passes Attempted'\n",
    "      AND league_name = 'SOCCER'\n",
    ") AS subquery\n",
    "WHERE\n",
    "    rn = 1\n",
    "    and description in (\n",
    "    \"Arsenal\",\n",
    "    \"Aston Villa\",\n",
    "    \"Bournemouth\",\n",
    "    \"Brentford\",\n",
    "    \"Brighton\",\n",
    "    \"Chelsea\",\n",
    "    \"Crystal Palace\",\n",
    "    \"Everton\",\n",
    "    \"Fulham\",\n",
    "    \"Ipswich\", # Promoted from Championship\n",
    "    \"Leicester\", # Promoted from Championship\n",
    "    \"Liverpool\",\n",
    "    \"Man City\",\n",
    "    \"Man Utd\",\n",
    "    \"Newcastle\"\n",
    "    \"Nottm Forest\",\n",
    "    \"Southampton\", # Promoted from Championship\n",
    "    \"Tottenham\",\n",
    "    \"West Ham\",\n",
    "    \"Wolves\"\n",
    ")\n",
    "and date(start_time) > date(2025, 4, 1)\n",
    "\"\"\"\n",
    "\n",
    "def execute_bq_query(query: str) -> pd.DataFrame:\n",
    "        client = bqc.Client(project='prizepicksanalytics')\n",
    "\n",
    "        query_results = client.query(query)\n",
    "        data = query_results.to_dataframe()\n",
    "        return data\n",
    "\n",
    "prizepicks_lines = execute_bq_query(prizepicks_lines_query)\n",
    "prizepicks_lines.start_time = prizepicks_lines.start_time.apply(lambda x: str(x)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca7bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = execute_bq_query(\"\"\"select * from soccer_simulations.match_calendar\"\"\")\n",
    "matches['home_team'] = matches.home_id.apply(lambda x: team_mappings[x])\n",
    "matches['away_team'] = matches.away_id.apply(lambda x: team_mappings[x])\n",
    "matches['description'] = matches.home_team + ' vs ' + matches.away_team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9585ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_team(row):\n",
    "    list_of_matches = matches[matches['localDate'] == row.start_time].description.to_list()\n",
    "    teams = [x for x in list_of_matches if row.description in x]\n",
    "\n",
    "    if len(teams) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    teams = teams[0].split(' vs ')\n",
    "    team = [t for t in teams if t != row.description][0]\n",
    "    return team\n",
    "\n",
    "prizepicks_lines['team'] = prizepicks_lines.apply(get_team, axis=1)\n",
    "prizepicks_lines = prizepicks_lines[~prizepicks_lines.team.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfafecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fuzzywuzzy\n",
    "from fuzzywuzzy import process, fuzz\n",
    "\n",
    "team_squads = {x:{k:v for k, v in zip(y.id, y.shortFirstName + ' ' + y.shortLastName)} for x, y in squads.groupby('contestantShortName')}\n",
    "test.player_name = test.apply(lambda x: team_squads[x.team_id][x.player_id], axis=1)\n",
    "\n",
    "def fuzzy_match(query, choices):\n",
    "    best_match, score = process.extractOne(query, choices, scorer=fuzz.token_set_ratio)\n",
    "    return best_match\n",
    "\n",
    "def get_fuzzy_name(row):\n",
    "    new_name = fuzzy_match(row.player_name, team_squads[row.team].values())\n",
    "    return new_name\n",
    "\n",
    "prizepicks_lines.player_name = prizepicks_lines.apply(lambda x: get_fuzzy_name(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d496cc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_eval = test.merge(prizepicks_lines, how='left', left_on=['player_name', 'match_date'], right_on=['player_name', 'start_time'])\n",
    "pp_eval = pp_eval[~pp_eval.line_score.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1309301",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_eval = pp_eval[['target', 'player_name', 'nb_mean', 'line_score', 'score', 'p_avg_passes', 'p_weighted_passes']]\n",
    "\n",
    "pp_eval['model_error'] = pp_eval.score - pp_eval.nb_mean\n",
    "pp_eval['pp_error'] = pp_eval.score - pp_eval.line_score\n",
    "\n",
    "model_mae = np.sum(np.abs(pp_eval['model_error'])) / pp_eval.shape[0]\n",
    "model_rmse = np.sqrt(np.sum(np.abs(pp_eval['model_error']))**2 / pp_eval.shape[0])\n",
    "\n",
    "pp_mae = np.sum(np.abs(pp_eval['pp_error'])) / pp_eval.shape[0]\n",
    "pp_rmse = np.sqrt(np.sum(np.abs(pp_eval['pp_error']))**2 / pp_eval.shape[0])\n",
    "\n",
    "print(f\"Model MAE: {model_mae} | Model RMSE: {model_rmse}\")\n",
    "print(f\"PP MAE: {pp_mae} | PP RMSE: {pp_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5dad7a",
   "metadata": {},
   "source": [
    "## misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cabf92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.groupby('player_position')['mae_error'].sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b540ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.groupby('player_position')['mae_error'].apply(lambda x: (sum(x)) / len(x)).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6778e1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.groupby('team_id')['error'].apply(lambda x: (sum(x)) / len(x)).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a38f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns # Often imported as sns\n",
    "\n",
    "# test.exp_std.plot()\n",
    "sns.scatterplot(\n",
    "    data=test[test['error'] < 60].iloc[:1000],\n",
    "    x='error',\n",
    "    y='avg_minsPlayed',\n",
    "    hue='player_position'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f72095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train[target].target.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6250e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.sort_values('target', ascending=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
